{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hostname: 6a6058df1898\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from surface_dice import SurfaceDiceMetric\n",
    "import albumentations as A\n",
    "import random\n",
    "import segmentation_models_pytorch as smp\n",
    "from patcher import Patcher\n",
    "\n",
    "hostname = os.uname().nodename\n",
    "print(\"Hostname:\", hostname)\n",
    "input_dir = \"data/blood-vessel-segmentation/\" if hostname == \"gamma\" else \"/kaggle/input/blood-vessel-segmentation/\"\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_dir = input_dir + \"train/\"\n",
    "\n",
    "# reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KidneyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgs_dir, msks_dir, slices_ids, transforms=None):\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.msks_dir = msks_dir\n",
    "        self.slices_ids = slices_ids\n",
    "        self.transforms = transforms\n",
    "        self.h = Image.open(imgs_dir + slices_ids[0]).height\n",
    "        self.w = Image.open(imgs_dir + slices_ids[0]).width\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slices_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slice_id = self.slices_ids[idx]\n",
    "        img_path = self.imgs_dir + slice_id\n",
    "        msk_path = self.msks_dir + slice_id\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        msk = Image.open(msk_path)\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        msk = np.array(msk)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            t = self.transforms(image=img, mask=msk)\n",
    "            img = t[\"image\"]\n",
    "            msk = t[\"mask\"]\n",
    "            \n",
    "        img = torch.from_numpy(img)[None, :]\n",
    "        msk = torch.as_tensor(msk)\n",
    "        img /= 31000\n",
    "        msk = msk // 255\n",
    "\n",
    "        return img, msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset length: 2279\n",
      "Eval Dataset length: 2279\n"
     ]
    }
   ],
   "source": [
    "imgs_dir = f\"{train_dir}kidney_1_dense/images/\"\n",
    "msks_dir = f\"{train_dir}kidney_1_dense/labels/\"\n",
    "slices_ids = sorted(os.listdir(imgs_dir))\n",
    "\n",
    "patch_size = 224\n",
    "transforms = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(patch_size, patch_size)\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_ds = KidneyDataset(\n",
    "    imgs_dir=imgs_dir,\n",
    "    msks_dir=msks_dir,\n",
    "    slices_ids=slices_ids,\n",
    "    transforms=transforms,\n",
    ")\n",
    "\n",
    "eval_ds = KidneyDataset(\n",
    "    imgs_dir=imgs_dir,\n",
    "    msks_dir=msks_dir,\n",
    "    slices_ids=slices_ids,\n",
    ")\n",
    "\n",
    "print(\"Train Dataset length:\", len(train_ds))\n",
    "print(\"Eval Dataset length:\", len(eval_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataLoader length: 72\n",
      "Eval DataLoader length: 143\n"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=32,\n",
    "    num_workers=8 if hostname == \"gamma\" else 2,\n",
    "    shuffle=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "eval_dl = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size=16,\n",
    "    num_workers=8 if hostname == \"gamma\" else 2,\n",
    "    shuffle=False,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "print(\"Train DataLoader length:\", len(train_dl))\n",
    "print(\"Eval DataLoader length:\", len(eval_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params: 2,881,625\n"
     ]
    }
   ],
   "source": [
    "net = smp.Unet(\n",
    "    encoder_name=\"timm-mobilenetv3_small_075\",\n",
    "    encoder_weights=None,\n",
    "    in_channels=1,\n",
    "    classes=1,\n",
    ")\n",
    "net.to(device)\n",
    "print(f\"Number of params: {sum([p.nelement() for p in net.parameters()]):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evalution pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random loss:\", -torch.tensor(1/2).log())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "optimizer = torch.optim.Adam(lr=lr, params=net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_loss = 0.0\n",
    "    net.train()\n",
    "    for x, y in tqdm(train_dl):\n",
    "        x, y = x.to(device), y.to(device).float()\n",
    "        logits = net(x).squeeze()\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_dl)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and Merge Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = eval_ds.h, eval_ds.w\n",
    "overlap = 50\n",
    "patcher = Patcher(h, w, patch_size=patch_size, overlap=overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval(save_preds=False):\n",
    "    eval_loss = 0.0\n",
    "    idx = 0\n",
    "    net.eval()\n",
    "    metric = SurfaceDiceMetric(n_batches=len(eval_dl), device=device)\n",
    "    for x, y in tqdm(eval_dl):\n",
    "        B, C, H, W = x.shape\n",
    "        x, y = x.to(device), y.to(device).float()\n",
    "        x = patcher.extract_patches(x)  # (B, n_patches, C, H, W)\n",
    "        x = x.flatten(end_dim=1)  # (B * n_patches, C, H, W)\n",
    "\n",
    "        logits = net(x)  # (B * n_patches, C, H, W)\n",
    "        logits = logits.unflatten(0, (B, -1))  # (B, n_patches, C, H, W)\n",
    "        logits = patcher.merge_patches(logits).squeeze()  # (B, H, W)\n",
    "\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        # save probabilities maps\n",
    "        if save_preds:\n",
    "            for i in range(bs):\n",
    "                Image.fromarray((logits.cpu()[i].sigmoid() * (2**16 - 1)).numpy().astype(np.uint16)).save(f\"preds/{idx:04}.tif\")\n",
    "                idx += 1\n",
    "\n",
    "        pred = torch.where(logits.sigmoid() >= 0.5, 1, 0)\n",
    "\n",
    "        metric.process_batch(pred, y)\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "    eval_loss /= len(eval_dl)\n",
    "    surface_dice = metric.compute()\n",
    "\n",
    "    return eval_loss, surface_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 40\n",
    "losses = []\n",
    "dices = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train()\n",
    "    print(f\"EPOCH {epoch}, TLOSS {train_loss:.4f}\")\n",
    "    losses.append(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/143 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 143/143 [01:11<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation\n",
      "ELOSS 0.0042, SURFACE_DICE 0.7274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_loss, surface_dice = eval(save_preds=False)\n",
    "print(\"Evaluation\")\n",
    "print(f\"ELOSS {eval_loss:.4f}, SURFACE_DICE {surface_dice:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), f\"checkpoints/baseline_train_sdc_{surface_dice:.3f}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
