{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from surface_dice import SurfaceDiceMetric\n",
    "import albumentations as A\n",
    "import random\n",
    "import segmentation_models_pytorch as smp\n",
    "from math import ceil\n",
    "from patcher import Patcher\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_dir = \"/kaggle/input/blood-vessel-segmentation/\"\n",
    "train_dir = input_dir + \"train/\"\n",
    "\n",
    "# reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KidneyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgs_dir, msks_dir, slices_ids, transforms=None):\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.msks_dir = msks_dir\n",
    "        self.slices_ids = slices_ids\n",
    "        self.transforms = transforms\n",
    "        self.h = Image.open(imgs_dir + slices_ids[0]).height\n",
    "        self.w = Image.open(imgs_dir + slices_ids[0]).width\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slices_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        slice_id = self.slices_ids[idx]\n",
    "        img_path = self.imgs_dir + slice_id\n",
    "        msk_path = self.msks_dir + slice_id\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        msk = Image.open(msk_path)\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        msk = np.array(msk)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            t = self.transforms(image=img, mask=msk)\n",
    "            img = t[\"image\"]\n",
    "            msk = t[\"mask\"]\n",
    "            \n",
    "        img = torch.from_numpy(img)[None, :]\n",
    "        msk = torch.as_tensor(msk, dtype=torch.float32)\n",
    "        img /= img.max() # [0, 1] range, hopefully max is not 0\n",
    "        msk /= 255 # {0, 1} values\n",
    "\n",
    "        return img, msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset length: 2279\n",
      "Eval Dataset length: 2279\n"
     ]
    }
   ],
   "source": [
    "imgs_dir = f\"{train_dir}kidney_1_dense/images/\"\n",
    "msks_dir = f\"{train_dir}kidney_1_dense/labels/\"\n",
    "slices_ids = sorted(os.listdir(imgs_dir))\n",
    "\n",
    "patch_size = 224\n",
    "transforms = A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(patch_size, patch_size)\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_ds = KidneyDataset(\n",
    "    imgs_dir=imgs_dir,\n",
    "    msks_dir=msks_dir,\n",
    "    slices_ids=slices_ids,\n",
    "    transforms=transforms,\n",
    ")\n",
    "\n",
    "eval_ds = KidneyDataset(\n",
    "    imgs_dir=imgs_dir,\n",
    "    msks_dir=msks_dir,\n",
    "    slices_ids=slices_ids,\n",
    ")\n",
    "\n",
    "print(\"Train Dataset length:\", len(train_ds))\n",
    "print(\"Eval Dataset length:\", len(eval_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataLoader length: 72\n",
      "Eval DataLoader length: 143\n"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=32,\n",
    "    num_workers=os.cpu_count(),\n",
    "    shuffle=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "eval_dl = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size=16,\n",
    "    num_workers=os.cpu_count(),\n",
    "    shuffle=False,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "print(\"Train DataLoader length:\", len(train_dl))\n",
    "print(\"Eval DataLoader length:\", len(eval_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params: 2,881,625\n"
     ]
    }
   ],
   "source": [
    "net = smp.Unet(\n",
    "    encoder_name=\"timm-mobilenetv3_small_075\",\n",
    "    encoder_weights=None,\n",
    "    in_channels=1,\n",
    "    classes=1,\n",
    ")\n",
    "net.to(device)\n",
    "print(f\"Number of params: {sum([p.nelement() for p in net.parameters()]):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evalution pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random loss: tensor(0.6931)\n"
     ]
    }
   ],
   "source": [
    "print(\"Random loss:\", -torch.tensor(1/2).log())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "optimizer = torch.optim.Adam(lr=lr, params=net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_loss = 0.0\n",
    "    net.train()\n",
    "    for x, y in tqdm(train_dl):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = net(x).squeeze()\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_dl)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and Merge Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ceil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/kaggle/working/blood-vessel-segmentation/baseline.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://aaf9-34-82-49-91.ngrok-free.app/kaggle/working/blood-vessel-segmentation/baseline.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m h, w \u001b[39m=\u001b[39m eval_ds\u001b[39m.\u001b[39mh, eval_ds\u001b[39m.\u001b[39mw\n\u001b[1;32m      <a href='vscode-notebook-cell://aaf9-34-82-49-91.ngrok-free.app/kaggle/working/blood-vessel-segmentation/baseline.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m overlap \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://aaf9-34-82-49-91.ngrok-free.app/kaggle/working/blood-vessel-segmentation/baseline.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m patcher \u001b[39m=\u001b[39m Patcher(h, w, patch_size\u001b[39m=\u001b[39;49mpatch_size, overlap\u001b[39m=\u001b[39;49moverlap)\n",
      "File \u001b[0;32m/kaggle/working/blood-vessel-segmentation/patcher.py:13\u001b[0m, in \u001b[0;36mPatcher.__init__\u001b[0;34m(self, h, w, patch_size, overlap)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moverlap \u001b[39m=\u001b[39m overlap\n\u001b[1;32m     12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride \u001b[39m=\u001b[39m patch_size \u001b[39m-\u001b[39m overlap\n\u001b[0;32m---> 13\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh_pad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride \u001b[39m*\u001b[39m ceil((h \u001b[39m-\u001b[39m patch_size) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride) \u001b[39m+\u001b[39m patch_size \u001b[39m-\u001b[39m h\n\u001b[1;32m     14\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw_pad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride \u001b[39m*\u001b[39m ceil((w \u001b[39m-\u001b[39m patch_size) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride) \u001b[39m+\u001b[39m patch_size \u001b[39m-\u001b[39m w\n\u001b[1;32m     16\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munfold \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mUnfold(\n\u001b[1;32m     17\u001b[0m     kernel_size\u001b[39m=\u001b[39m(patch_size, patch_size),\n\u001b[1;32m     18\u001b[0m     stride\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride\n\u001b[1;32m     19\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ceil' is not defined"
     ]
    }
   ],
   "source": [
    "h, w = eval_ds.h, eval_ds.w\n",
    "overlap = 50\n",
    "patcher = Patcher(h, w, patch_size=patch_size, overlap=overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval():\n",
    "    eval_loss = 0.0\n",
    "    idx = 0\n",
    "    net.eval()\n",
    "    metric = SurfaceDiceMetric(n_batches=len(eval_dl), device=device)\n",
    "    for x, y in tqdm(eval_dl):\n",
    "        bs = len(x)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x = patcher.extract_patches(x)  # (bs, n_patches, h, w)\n",
    "\n",
    "        logits = net(x.reshape(-1, 1, patch_size, patch_size))  # (bs * n_patches, 1, patch_size, patch_size)\n",
    "        logits = logits.view(bs, -1, patch_size, patch_size)  # (bs, n_patches, patch_size, patch_size)\n",
    "        logits = patcher.merge_patches(logits).squeeze()  # (bs, h, w)\n",
    "\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        # save probabilities maps\n",
    "        for i in range(bs):\n",
    "            Image.fromarray((logits.cpu()[i].sigmoid() * (2**16 - 1)).numpy().astype(np.uint16)).save(f\"preds/{idx:04}.tif\")\n",
    "            idx += 1\n",
    "\n",
    "        pred = torch.where(logits.sigmoid() >= 0.5, 1, 0)\n",
    "\n",
    "        metric.process_batch(pred, y)\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "    eval_loss /= len(eval_dl)\n",
    "    surface_dice = metric.compute()\n",
    "\n",
    "    return eval_loss, surface_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 40\n",
    "losses = []\n",
    "dices = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train()\n",
    "    print(f\"EPOCH {epoch}, TLOSS {train_loss:.4f}\")\n",
    "    losses.append(train_loss)\n",
    "\n",
    "eval_loss, surface_dice = eval()\n",
    "print()\n",
    "print(f\"ELOSS {eval_loss:.4f}, SURFACE_DICE {surface_dice:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"baseline.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
